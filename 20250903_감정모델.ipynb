{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2025913",
   "metadata": {},
   "source": [
    "# 사전학습 모델 cardiffnlp/twitter-roberta-base-sentiment 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ccbbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.55.4)\n",
      "Requirement already satisfied: torch in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\MYNOTE\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fdc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selecting examples: 100%|██████████| 2029/2029 [00:09<00:00, 216.00it/s]\n",
      "Selecting examples: 100%|██████████| 10/10 [00:00<00:00, 223.30it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7c5afed0944bb89739cb56d489e103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MYNOTE\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MYNOTE\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caec8c0b09d942c2b555cbad30475c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde6f1a1b1214ffda8702ed1fad7cc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f54b23e5f44f63be1c3ad74f16fe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7cea9f56554cdd89c355fec6d06548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring:   2%|▏         | 2/127 [00:06<06:37,  3.18s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Inferring:   2%|▏         | 3/127 [00:09<06:46,  3.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffd3c96b9db46c4aa18b10d80b04b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferring: 100%|██████████| 127/127 [09:08<00:00,  4.32s/it]\n",
      "Inferring: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n",
      "Inferring: 100%|██████████| 127/127 [04:32<00:00,  2.14s/it]\n",
      "Inferring: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] train_with_sentiment_ctx.csv (shape=(2029, 14))\n",
      "[Saved] test_with_sentiment_ctx.csv  (shape=(10, 13))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, re, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===============================\n",
    "# 0) Config\n",
    "# ===============================\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "MAX_LEN_CTX = 320          # 컨텍스트까지 넣을 때 토큰 길이\n",
    "MAX_LEN_PLAIN = 256        # body+subreddit만 넣을 때 토큰 길이\n",
    "BATCH_SIZE = 16\n",
    "USE_BLEND = True           # 컨텍스트+플레인 확률 가중 평균\n",
    "BLEND_W_CTX = 0.7          # 0.7*ctx + 0.3*plain\n",
    "\n",
    "INPUT_TRAIN = \"train.csv\"\n",
    "INPUT_TEST  = \"test.csv\"\n",
    "\n",
    "# ===============================\n",
    "# 1) Load\n",
    "# ===============================\n",
    "train = pd.read_csv(INPUT_TRAIN)\n",
    "test  = pd.read_csv(INPUT_TEST)\n",
    "\n",
    "REQUIRED = [\"body\", \"subreddit\",\n",
    "            \"positive_example_1\", \"positive_example_2\",\n",
    "            \"negative_example_1\", \"negative_example_2\"]\n",
    "for c in REQUIRED:\n",
    "    if c not in train.columns:\n",
    "        raise ValueError(f\"train.csv에 '{c}' 컬럼이 필요합니다.\")\n",
    "    if c not in test.columns:\n",
    "        # test엔 rule_violation, rule 등이 없어도 되지만 예시 컬럼은 있다고 가정\n",
    "        raise ValueError(f\"test.csv에 '{c}' 컬럼이 필요합니다.\")\n",
    "\n",
    "def _clean(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    # 트위터/레딧 스타일 텍스트 보존, 과도한 정규화는 피함\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in [\"body\",\"subreddit\",\n",
    "              \"positive_example_1\",\"positive_example_2\",\n",
    "              \"negative_example_1\",\"negative_example_2\"]:\n",
    "        out[c] = out[c].map(_clean)\n",
    "    return out\n",
    "\n",
    "train = preprocess_df(train)\n",
    "test  = preprocess_df(test)\n",
    "\n",
    "# ===============================\n",
    "# 2) 유사한 예시 자동 선택 (row 단위)\n",
    "#    - char n-gram TF-IDF로 body와 각 예시 간 유사도를 계산해\n",
    "#      positive/negative 각각 최적 예시 1개씩 고름\n",
    "# ===============================\n",
    "def pick_best_example(body: str, ex1: str, ex2: str) -> str:\n",
    "    cands = [ex1 or \"\", ex2 or \"\"]\n",
    "    # 간단하지만 강력한 char_wb n-gram → URL/오탈자/이모지에 비교적 강함\n",
    "    vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\n",
    "    X = vec.fit_transform([body] + cands)\n",
    "    sims = cosine_similarity(X[0], X[1:]).flatten()\n",
    "    best_idx = int(np.argmax(sims)) if len(sims) else 0\n",
    "    return cands[best_idx]\n",
    "\n",
    "def build_context_text(subreddit: str, body: str,\n",
    "                       pos_ex: str, neg_ex: str) -> str:\n",
    "    # RoBERTa에 직접 few-shot 힌트를 주는 형식 (간결하고 일관되게)\n",
    "    return (\n",
    "        f\"subreddit: {subreddit} [SEP] \"\n",
    "        f\"body: {body} [SEP] \"\n",
    "        f\"pos_example: {pos_ex} [SEP] \"\n",
    "        f\"neg_example: {neg_ex}\"\n",
    "    )\n",
    "\n",
    "def build_plain_text(subreddit: str, body: str) -> str:\n",
    "    return f\"subreddit: {subreddit} [SEP] body: {body}\"\n",
    "\n",
    "def make_text_inputs(df: pd.DataFrame):\n",
    "    ctx_texts, plain_texts = [], []\n",
    "    for _, r in tqdm(df.iterrows(), total=len(df), desc=\"Selecting examples\"):\n",
    "        pos_best = pick_best_example(r[\"body\"], r[\"positive_example_1\"], r[\"positive_example_2\"])\n",
    "        neg_best = pick_best_example(r[\"body\"], r[\"negative_example_1\"], r[\"negative_example_2\"])\n",
    "        ctx_texts.append(build_context_text(r[\"subreddit\"], r[\"body\"], pos_best, neg_best))\n",
    "        plain_texts.append(build_plain_text(r[\"subreddit\"], r[\"body\"]))\n",
    "    return ctx_texts, plain_texts\n",
    "\n",
    "train_ctx, train_plain = make_text_inputs(train)\n",
    "test_ctx,  test_plain  = make_text_inputs(test)\n",
    "\n",
    "# ===============================\n",
    "# 3) 모델 로드\n",
    "# ===============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "labels = np.array([\"negative\",\"neutral\",\"positive\"])\n",
    "\n",
    "# ===============================\n",
    "# 4) 배치 추론 함수\n",
    "# ===============================\n",
    "@torch.no_grad()\n",
    "def predict_texts(texts, max_len) -> np.ndarray:\n",
    "    probs_all = []\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Inferring\"):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        inputs = tokenizer(\n",
    "            batch, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=max_len\n",
    "        ).to(device)\n",
    "        logits = model(**inputs).logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()\n",
    "        probs_all.append(probs)\n",
    "        del inputs, logits\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    return np.vstack(probs_all)\n",
    "\n",
    "# 컨텍스트/플레인 모두 추론\n",
    "train_probs_ctx  = predict_texts(train_ctx,  MAX_LEN_CTX)\n",
    "test_probs_ctx   = predict_texts(test_ctx,   MAX_LEN_CTX)\n",
    "\n",
    "train_probs_plain = predict_texts(train_plain, MAX_LEN_PLAIN)\n",
    "test_probs_plain  = predict_texts(test_plain,  MAX_LEN_PLAIN)\n",
    "\n",
    "# ===============================\n",
    "# 5) 블렌딩 \n",
    "# ===============================\n",
    "def blend(p_ctx, p_plain, w_ctx=0.7):\n",
    "    return w_ctx * p_ctx + (1.0 - w_ctx) * p_plain\n",
    "\n",
    "if USE_BLEND:\n",
    "    train_probs = blend(train_probs_ctx, train_probs_plain, BLEND_W_CTX)\n",
    "    test_probs  = blend(test_probs_ctx,  test_probs_plain,  BLEND_W_CTX)\n",
    "else:\n",
    "    train_probs = train_probs_ctx\n",
    "    test_probs  = test_probs_ctx\n",
    "\n",
    "# 예측 라벨/신뢰도\n",
    "train_pred_idx = train_probs.argmax(1)\n",
    "test_pred_idx  = test_probs.argmax(1)\n",
    "\n",
    "train_pred = labels[train_pred_idx]\n",
    "test_pred  = labels[test_pred_idx]\n",
    "\n",
    "train_conf = train_probs.max(1)\n",
    "test_conf  = test_probs.max(1)\n",
    "\n",
    "# ===============================\n",
    "# 6) 저장\n",
    "# ===============================\n",
    "train_out = train.copy()\n",
    "train_out[\"sentiment\"] = train_pred\n",
    "train_out[\"confidence\"] = train_conf\n",
    "for i, name in enumerate(labels):\n",
    "    train_out[f\"prob_{name}\"] = train_probs[:, i]\n",
    "\n",
    "test_out = test.copy()\n",
    "test_out[\"sentiment\"] = test_pred\n",
    "test_out[\"confidence\"] = test_conf\n",
    "for i, name in enumerate(labels):\n",
    "    test_out[f\"prob_{name}\"] = test_probs[:, i]\n",
    "\n",
    "train_save = \"train_with_sentiment_ctx.csv\"\n",
    "test_save  = \"test_with_sentiment_ctx.csv\"\n",
    "train_out.to_csv(train_save, index=False)\n",
    "test_out.to_csv(test_save, index=False)\n",
    "\n",
    "print(f\"[Saved] {train_save} (shape={train_out.shape})\")\n",
    "print(f\"[Saved] {test_save}  (shape={test_out.shape})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd043f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AUC Summary ===\n",
      "         score_name  valid_ROC_AUC\n",
      "0   meta_logreg_OOF       0.575256\n",
      "1          neg_only       0.569192\n",
      "2     neg_minus_pos       0.568118\n",
      "3  neg_plus_halfneu       0.568118\n",
      "4        1-pos_only       0.563397\n",
      "\n",
      "[Saved] submission: /mnt/data/submission_from_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "# Re-run AUC evaluation now that the user has uploaded the needed CSVs.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# 1) Load data\n",
    "train_raw = pd.read_csv(\"train.csv\")\n",
    "train_pred = pd.read_csv(\"train_with_sentiment_ctx.csv\")\n",
    "test_pred  = pd.read_csv(\"test_with_sentiment_ctx.csv\")\n",
    "\n",
    "# 2) Sanity checks\n",
    "assert \"row_id\" in train_raw.columns and \"row_id\" in train_pred.columns, \"row_id column required\"\n",
    "need = [\"prob_negative\",\"prob_neutral\",\"prob_positive\"]\n",
    "for c in need:\n",
    "    assert c in train_pred.columns, f\"Missing column in train predictions: {c}\"\n",
    "    assert c in test_pred.columns, f\"Missing column in test predictions: {c}\"\n",
    "\n",
    "# 3) Merge & prepare labels\n",
    "df = train_raw.merge(train_pred[[\"row_id\"]+need], on=\"row_id\", how=\"inner\").copy()\n",
    "y = df[\"rule_violation\"].astype(int).values\n",
    "\n",
    "# 4) Heuristic scores and their AUCs\n",
    "scores = {\n",
    "    \"neg_only\":            df[\"prob_negative\"].values,\n",
    "    \"1-pos_only\":          1.0 - df[\"prob_positive\"].values,\n",
    "    \"neg_minus_pos\":       (df[\"prob_negative\"] - df[\"prob_positive\"]).values,\n",
    "    \"neg_plus_halfneu\":    (df[\"prob_negative\"] + 0.5*df[\"prob_neutral\"]).values,\n",
    "}\n",
    "\n",
    "auc_rows = []\n",
    "for name, s in scores.items():\n",
    "    auc = roc_auc_score(y, s)\n",
    "    auc_rows.append({\"score_name\": name, \"valid_ROC_AUC\": auc})\n",
    "\n",
    "# 5) Meta-model (logistic regression) with OOF AUC\n",
    "X = df[need].values\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "meta = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "oof_proba = cross_val_predict(meta, X, y, cv=cv, method=\"predict_proba\")[:,1]\n",
    "oof_auc = roc_auc_score(y, oof_proba)\n",
    "auc_rows.append({\"score_name\": \"meta_logreg_OOF\", \"valid_ROC_AUC\": oof_auc})\n",
    "\n",
    "auc_table = pd.DataFrame(auc_rows).sort_values(\"valid_ROC_AUC\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 6) Fit on full train and predict test, save submission\n",
    "meta.fit(X, y)\n",
    "X_test = test_pred[need].values\n",
    "test_rule_violation = meta.predict_proba(X_test)[:,1]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": test_pred[\"row_id\"],\n",
    "    \"rule_violation\": test_rule_violation\n",
    "})\n",
    "sub_path = \"/mnt/data/submission_from_sentiment.csv\"\n",
    "submission.to_csv(sub_path, index=False)\n",
    "\n",
    "# 7) Show the AUC table to the user and print path\n",
    "\n",
    "print(\"=== AUC Summary ===\")\n",
    "print(auc_table)\n",
    "print(f\"\\n[Saved] submission: {sub_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
