{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61855e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 예측을 생성 중입니다...\n",
      "A_logreg OOF AUC: 0.8385\n",
      "B_sgd_svm_cal OOF AUC: 0.7905\n",
      "C_gb_heur OOF AUC: 0.7246\n",
      "최적의 블렌딩 가중치를 찾는 중입니다...\n",
      "Best blend AUC: 0.8385 with weights (A,B,C) = (1.0, 0.0, 0.0)\n",
      "전체 데이터로 모델을 학습하고 테스트 데이터를 예측하는 중입니다...\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Ensemble for AUC (warnings-free version)\n",
    "#  - Model A: TF-IDF(word+char)+Heuristic -> Logistic\n",
    "#  - Model B: TF-IDF(word+char)+Heuristic -> SGDClassifier (hinge) + Calibrated (sigmoid)\n",
    "#  - Model C: Heuristic-only -> GradientBoosting\n",
    "#  - Blend weights optimized by OOF AUC (with numerical clamp)\n",
    "#  - Predict test and build my_submission (probabilities)\n",
    "# ============================================\n",
    "import re, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from scipy.sparse import csr_matrix\n",
    "# ---- Optional: suppress convergence warnings (we also fixed root cause) ----\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # FutureWarning도 무시하도록 추가\n",
    "# --------- Paths ----------\n",
    "PATH_TRAIN = \"train.csv\"\n",
    "PATH_TEST  = \"test.csv\"\n",
    "# --------- Text column detection ----------\n",
    "TEXT_CANDIDATES = [\"body\",\"text\",\"comment\",\"comment_text\",\"content\"]\n",
    "def guess_text_col(df: pd.DataFrame):\n",
    "    for c in TEXT_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"텍스트 컬럼을 찾지 못했습니다. 후보: {TEXT_CANDIDATES}\")\n",
    "# --------- Normalization (obfuscation aware) ----------\n",
    "def text_obfuscation_normalize(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\",\" \", s)\n",
    "    table = str.maketrans({\"0\":\"o\",\"1\":\"i\",\"3\":\"e\",\"4\":\"a\",\"5\":\"s\",\"7\":\"t\",\"$\":\"s\",\"@\":\"a\",\"!\":\"i\",\"+\":\"t\"})\n",
    "    s = s.translate(table)\n",
    "    s = re.sub(r\"([!?.,])\\1{2,}\", r\"\\1\\1\", s)\n",
    "    # s.h.i.t -> shit\n",
    "    s = re.sub(r\"(?:\\b|_)([a-z])(?:[.\\-_/ ]+[a-z])+\", lambda m: m.group(0).replace(\".\",\"\").replace(\"-\",\"\").replace(\"_\",\"\").replace(\"/\",\"\").replace(\" \",\"\"), s)\n",
    "    return s\n",
    "def normalize_simple(s: str) -> str:\n",
    "    s = text_obfuscation_normalize(s)\n",
    "    s = re.sub(r\"[^a-z0-9'@/:._-]+\", \" \", s)\n",
    "    return s.strip()\n",
    "# --------- Heuristic features (from analysis) ----------\n",
    "POS = {\"good\",\"great\",\"awesome\",\"amazing\",\"love\",\"like\",\"nice\",\"kind\",\"helpful\",\"thanks\",\"thank\",\"appreciate\",\n",
    "       \"brilliant\",\"fantastic\",\"excellent\",\"well\",\"welldone\",\"congrats\",\"congratulations\",\"cool\",\"sweet\",\"cheers\",\n",
    "       \"solid\",\"decent\",\"smart\",\"interesting\",\"insightful\",\"clear\",\"accurate\",\"agree\",\"true\",\"valid\",\"correct\"}\n",
    "NEG = {\"bad\",\"worse\",\"worst\",\"hate\",\"hates\",\"hated\",\"awful\",\"terrible\",\"horrible\",\"disgusting\",\"gross\",\"stupid\",\n",
    "       \"idiot\",\"idiotic\",\"moron\",\"dumb\",\"dumber\",\"dumbest\",\"trash\",\"garbage\",\"sucks\",\"loser\",\"pathetic\",\"toxic\",\n",
    "       \"annoying\",\"angry\",\"mad\",\"ridiculous\",\"bs\",\"nonsense\",\"spam\",\"fake\",\"liar\",\"lying\",\"lie\",\"lies\",\"ignorant\",\n",
    "       \"ugly\",\"useless\",\"pointless\",\"wrong\",\"incorrect\",\"false\",\"misleading\",\"cringe\",\"cringy\",\"wtf\",\"wtheck\",\n",
    "       \"shut\",\"shutup\",\"goaway\",\"hell\",\"nazi\",\"racist\",\"sexist\",\"homophobic\",\"offensive\",\"rude\",\"jerk\"}\n",
    "PROF = {\"fuck\",\"fucking\",\"fucked\",\"shit\",\"bullshit\",\"bitch\",\"bitches\",\"ass\",\"asses\",\"asshole\",\"dick\",\"dicks\",\"prick\",\n",
    "        \"bastard\",\"crap\",\"cunt\",\"piss\",\"pissed\",\"damn\",\"goddamn\",\"mf\",\"motherfucker\",\"retard\",\"retarded\",\"dumbass\",\n",
    "        \"screw\",\"screwyou\"}\n",
    "PA   = {\"idiot\",\"idiotic\",\"moron\",\"jerk\",\"loser\",\"stupid\",\"dumb\",\"dumbass\",\"asshole\",\"bastard\",\"prick\"}\n",
    "HATE = {\"nazi\",\"racist\",\"sexist\",\"homophobic\",\"retard\",\"retarded\"}\n",
    "LAW  = {\"sue\",\"lawyer\",\"attorney\",\"police\",\"court\",\"illegal\",\"legal\",\"legally\",\"code\",\"codes\"}\n",
    "MOD  = {\"mod\",\"mods\",\"moderator\",\"moderators\",\"ban\",\"banned\",\"unban\"}\n",
    "NEG_PH = {\"screw you\",\"shut up\",\"kill yourself\",\"die in a hole\",\"go to hell\"}\n",
    "def cnt(tokens, S):\n",
    "    return sum(1 for w in tokens if w in S)\n",
    "class HeuristicFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"text -> csr_matrix of engineered features (standardized)\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_  = None\n",
    "    def fit(self, X, y=None):\n",
    "        F = self._featurize_many(X).toarray()\n",
    "        self.mean_ = F.mean(axis=0)\n",
    "        self.std_ = F.std(axis=0) + 1e-6\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        F = self._featurize_many(X).toarray()\n",
    "        F = (F - self.mean_) / self.std_\n",
    "        return csr_matrix(F)\n",
    "    def _featurize_many(self, X):\n",
    "        rows = []\n",
    "        for s in X:\n",
    "            raw = str(s)\n",
    "            norm = normalize_simple(raw)\n",
    "            toks = norm.split()\n",
    "            pos_cnt = cnt(toks, POS)\n",
    "            neg_cnt = cnt(toks, NEG)\n",
    "            prof_cnt = cnt(toks, PROF)\n",
    "            pa_cnt   = cnt(toks, PA)\n",
    "            hate_cnt = cnt(toks, HATE)\n",
    "            law_cnt  = cnt(toks, LAW)\n",
    "            mod_cnt  = cnt(toks, MOD)\n",
    "            neg_phrase_flag = int(any(p in norm for p in NEG_PH))\n",
    "            sent_score = (pos_cnt - neg_cnt) / (1 + pos_cnt + neg_cnt)\n",
    "            word_count = len(toks)\n",
    "            char_len   = len(raw)\n",
    "            exclaim_cnt= raw.count(\"!\")\n",
    "            upper_ratio= sum(1 for ch in raw if ch.isupper())/max(1,len(raw))\n",
    "            has_url    = 1 if re.search(r\"http[s]?://\", raw, flags=re.I) else 0\n",
    "            has_mention= 1 if re.search(r\"(?:\\bu/|@[A-Za-z0-9_]+)\", raw) else 0\n",
    "            neg_x_excl = neg_cnt * exclaim_cnt\n",
    "            profanity_pos = prof_cnt * int(sent_score>0)\n",
    "            law_short = law_cnt * int(word_count <= 12)\n",
    "            url_short = has_url * int(word_count <= 12)\n",
    "            rows.append([\n",
    "                sent_score, pos_cnt, neg_cnt, prof_cnt, pa_cnt, hate_cnt, law_cnt, mod_cnt,\n",
    "                neg_phrase_flag, word_count, char_len, exclaim_cnt, upper_ratio, has_url, has_mention,\n",
    "                neg_x_excl, profanity_pos, law_short, url_short\n",
    "            ])\n",
    "        return csr_matrix(np.array(rows, dtype=np.float32))\n",
    "# --------- Load data ----------\n",
    "train = pd.read_csv(PATH_TRAIN)\n",
    "test  = pd.read_csv(PATH_TEST)\n",
    "assert \"rule_violation\" in train.columns, \"train.csv에 'rule_violation' 컬럼이 필요합니다.\"\n",
    "text_col = guess_text_col(train)\n",
    "assert text_col in test.columns, f\"test.csv에도 '{text_col}' 컬럼이 필요합니다.\"\n",
    "if \"row_id\" not in test.columns:\n",
    "    test[\"row_id\"] = np.arange(len(test))\n",
    "X_train = pd.DataFrame({\"text\": train[text_col].fillna(\"\").astype(str)})\n",
    "X_test  = pd.DataFrame({\"text\":  test[text_col].fillna(\"\").astype(str)})\n",
    "y = pd.to_numeric(train[\"rule_violation\"], errors=\"coerce\").fillna(0).astype(int).values\n",
    "# --------- Shared preprocessors ----------\n",
    "prep_wordchar_heur = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"word\", TfidfVectorizer(ngram_range=(1,2), min_df=3, token_pattern=r\"[A-Za-z']{2,}\"), \"text\"),\n",
    "        (\"char\", TfidfVectorizer(analyzer=\"char\", ngram_range=(2,5), min_df=3), \"text\"),\n",
    "        (\"heur\", Pipeline([(\"feat\", HeuristicFeaturizer())]), \"text\"),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "prep_heur_only = ColumnTransformer(\n",
    "    transformers=[(\"heur\", Pipeline([(\"feat\", HeuristicFeaturizer())]), \"text\")],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "# --------- Models ----------\n",
    "model_A = Pipeline([\n",
    "    (\"prep\", prep_wordchar_heur),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, C=2.0))\n",
    "])\n",
    "\n",
    "model_B = Pipeline([\n",
    "    (\"prep\", prep_wordchar_heur),\n",
    "    (\"clf\", CalibratedClassifierCV(\n",
    "        estimator=SGDClassifier(     \n",
    "            loss=\"hinge\",             # SVM hinge loss\n",
    "            alpha=1e-5,               # L2 정규화 강도 (튜닝 포인트)\n",
    "            max_iter=3000,\n",
    "            tol=1e-3,\n",
    "            random_state=42\n",
    "        ),\n",
    "        cv=3, method=\"sigmoid\"\n",
    "    ))\n",
    "])\n",
    "model_C = Pipeline([\n",
    "    (\"prep\", prep_heur_only),\n",
    "    (\"clf\", GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "models = {\"A_logreg\": model_A, \"B_sgd_svm_cal\": model_B, \"C_gb_heur\": model_C}\n",
    "# --------- OOF predictions for weight search ----------\n",
    "print(\"OOF 예측을 생성 중입니다...\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = {}\n",
    "for name, pipe in models.items():\n",
    "    oof[name] = cross_val_predict(pipe, X_train, y, cv=skf, method=\"predict_proba\")[:,1]\n",
    "    print(f\"{name} OOF AUC: {roc_auc_score(y, oof[name]):.4f}\")\n",
    "# --------- Simple grid search for blend weights (clamped) ----------\n",
    "print(\"최적의 블렌딩 가중치를 찾는 중입니다...\")\n",
    "best_auc, best_w = -1.0, (1.0, 0.0, 0.0)\n",
    "weights = np.arange(0.0, 1.01, 0.05)\n",
    "for w1 in weights:\n",
    "    for w2 in weights:\n",
    "        w3 = 1.0 - (w1 + w2)\n",
    "        if w3 < -1e-9:    # infeasible\n",
    "            continue\n",
    "        w3 = max(0.0, w3) # clamp tiny negatives to 0\n",
    "        blend = w1*oof[\"A_logreg\"] + w2*oof[\"B_sgd_svm_cal\"] + w3*oof[\"C_gb_heur\"]\n",
    "        auc = roc_auc_score(y, blend)\n",
    "        if auc > best_auc:\n",
    "            best_auc, best_w = auc, (float(w1), float(w2), float(w3))\n",
    "print(f\"Best blend AUC: {best_auc:.4f} with weights (A,B,C) = {best_w}\")\n",
    "# --------- Fit all on full data & predict test ----------\n",
    "print(\"전체 데이터로 모델을 학습하고 테스트 데이터를 예측하는 중입니다...\")\n",
    "fitted = {name: clone(pipe).fit(X_train, y) for name, pipe in models.items()}\n",
    "test_pred = {name: est.predict_proba(X_test)[:,1] for name, est in fitted.items()}\n",
    "w1, w2, w3 = best_w\n",
    "responses = w1*test_pred[\"A_logreg\"] + w2*test_pred[\"B_sgd_svm_cal\"] + w3*test_pred[\"C_gb_heur\"]\n",
    "my_submission = pd.DataFrame({\n",
    "    'row_id': test['row_id'],\n",
    "    'rule_violation': responses\n",
    "})\n",
    "#my_submission.to_csv('submission.csv', index=False)\n",
    "#print(\"submission.csv 파일 생성이 완료되었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
