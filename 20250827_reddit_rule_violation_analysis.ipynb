{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86cdb0b",
   "metadata": {},
   "source": [
    "# 🧠 Reddit Rule Violation 분류 프로젝트\n",
    "\n",
    "이 노트북은 Reddit 댓글이 주어진 규칙을 위반했는지를 예측하는 이진 분류 과제를 위한 실험 전체를 정리한 것입니다.\n",
    "\n",
    "## 📌 주요 목표\n",
    "- 댓글(`body`)과 규칙(`rule`) 간 의미적 유사도 기반 feature 생성\n",
    "- positive/negative 예시와의 유사도(`pos_sim`, `neg_sim`)를 feature로 추가\n",
    "- 간단한 모델(LogisticRegression)과 BERT fine-tuning 실험\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tqdm.pandas()\n",
    "\n",
    "# 유사도 계산 함수\n",
    "def compute_similarities(row):\n",
    "    body_vec = model.encode(row['body'], convert_to_tensor=True)\n",
    "    rule_vec = model.encode(row['rule'], convert_to_tensor=True)\n",
    "    pos_sim = (\n",
    "        util.cos_sim(body_vec, model.encode(row['positive_example_1'], convert_to_tensor=True)) +\n",
    "        util.cos_sim(body_vec, model.encode(row['positive_example_2'], convert_to_tensor=True))\n",
    "    ) / 2\n",
    "    neg_sim = (\n",
    "        util.cos_sim(body_vec, model.encode(row['negative_example_1'], convert_to_tensor=True)) +\n",
    "        util.cos_sim(body_vec, model.encode(row['negative_example_2'], convert_to_tensor=True))\n",
    "    ) / 2\n",
    "    return pd.Series({\n",
    "        'body_rule_similarity': util.cos_sim(body_vec, rule_vec).item(),\n",
    "        'pos_sim': pos_sim.item(),\n",
    "        'neg_sim': neg_sim.item()\n",
    "    })\n",
    "\n",
    "# 적용\n",
    "df[['body_rule_similarity', 'pos_sim', 'neg_sim']] = df.progress_apply(compute_similarities, axis=1)\n",
    "\n",
    "# BERT 입력용 텍스트 생성\n",
    "df['bert_input'] = \"[RULE] \" + df['rule'] + \" [SEP] \" + df['body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95894474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 히스토그램\n",
    "sns.histplot(df['body_rule_similarity'], bins=30, kde=True)\n",
    "plt.title(\"Body-Rule Similarity Distribution\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 박스플롯\n",
    "sns.boxplot(x='rule_violation', y='body_rule_similarity', data=df)\n",
    "plt.title(\"Similarity by Rule Violation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ab496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "X = df[['body_rule_similarity', 'pos_sim', 'neg_sim']]\n",
    "y = df['rule_violation']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y, y_prob))\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['bert_input'], df['rule_violation'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} | {'labels': torch.tensor(self.labels[idx])}\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RedditDataset(train_encodings, y_train)\n",
    "val_dataset = RedditDataset(val_encodings, y_val)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 평가\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "y_pred = preds_output.predictions.argmax(-1)\n",
    "y_true = y_val.values\n",
    "probs = preds_output.predictions[:, 1]\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, probs))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}