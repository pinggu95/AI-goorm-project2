{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86cdb0b",
   "metadata": {},
   "source": [
    "# ğŸ§  Reddit Rule Violation ë¶„ë¥˜ í”„ë¡œì íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Reddit ëŒ“ê¸€ì´ ì£¼ì–´ì§„ ê·œì¹™ì„ ìœ„ë°˜í–ˆëŠ”ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ê³¼ì œë¥¼ ìœ„í•œ ì‹¤í—˜ ì „ì²´ë¥¼ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“Œ ì£¼ìš” ëª©í‘œ\n",
    "- ëŒ“ê¸€(`body`)ê³¼ ê·œì¹™(`rule`) ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ feature ìƒì„±\n",
    "- positive/negative ì˜ˆì‹œì™€ì˜ ìœ ì‚¬ë„(`pos_sim`, `neg_sim`)ë¥¼ featureë¡œ ì¶”ê°€\n",
    "- ê°„ë‹¨í•œ ëª¨ë¸(LogisticRegression)ê³¼ BERT fine-tuning ì‹¤í—˜\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tqdm.pandas()\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜\n",
    "def compute_similarities(row):\n",
    "    body_vec = model.encode(row['body'], convert_to_tensor=True)\n",
    "    rule_vec = model.encode(row['rule'], convert_to_tensor=True)\n",
    "    pos_sim = (\n",
    "        util.cos_sim(body_vec, model.encode(row['positive_example_1'], convert_to_tensor=True)) +\n",
    "        util.cos_sim(body_vec, model.encode(row['positive_example_2'], convert_to_tensor=True))\n",
    "    ) / 2\n",
    "    neg_sim = (\n",
    "        util.cos_sim(body_vec, model.encode(row['negative_example_1'], convert_to_tensor=True)) +\n",
    "        util.cos_sim(body_vec, model.encode(row['negative_example_2'], convert_to_tensor=True))\n",
    "    ) / 2\n",
    "    return pd.Series({\n",
    "        'body_rule_similarity': util.cos_sim(body_vec, rule_vec).item(),\n",
    "        'pos_sim': pos_sim.item(),\n",
    "        'neg_sim': neg_sim.item()\n",
    "    })\n",
    "\n",
    "# ì ìš©\n",
    "df[['body_rule_similarity', 'pos_sim', 'neg_sim']] = df.progress_apply(compute_similarities, axis=1)\n",
    "\n",
    "# BERT ì…ë ¥ìš© í…ìŠ¤íŠ¸ ìƒì„±\n",
    "df['bert_input'] = \"[RULE] \" + df['rule'] + \" [SEP] \" + df['body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95894474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# íˆìŠ¤í† ê·¸ë¨\n",
    "sns.histplot(df['body_rule_similarity'], bins=30, kde=True)\n",
    "plt.title(\"Body-Rule Similarity Distribution\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# ë°•ìŠ¤í”Œë¡¯\n",
    "sns.boxplot(x='rule_violation', y='body_rule_similarity', data=df)\n",
    "plt.title(\"Similarity by Rule Violation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ab496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "X = df[['body_rule_similarity', 'pos_sim', 'neg_sim']]\n",
    "y = df['rule_violation']\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y, y_prob))\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['bert_input'], df['rule_violation'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} | {'labels': torch.tensor(self.labels[idx])}\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RedditDataset(train_encodings, y_train)\n",
    "val_dataset = RedditDataset(val_encodings, y_val)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT í‰ê°€\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "preds_output = trainer.predict(val_dataset)\n",
    "y_pred = preds_output.predictions.argmax(-1)\n",
    "y_true = y_val.values\n",
    "probs = preds_output.predictions[:, 1]\n",
    "\n",
    "print(\"ROC AUC:\", roc_auc_score(y_true, probs))\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}