{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737065a2-3695-4342-bddb-e46f5247660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.1.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.25.1)\n",
      "Collecting scipy (from xgboost)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m245.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m498.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, xgboost, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0 xgboost-3.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2372eb6-34bc-43bc-8b9b-02704208c99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m280.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3a3e5b-0ca3-42ef-8abe-df9c223a2cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67862ba-d2aa-4207-8d3a-bb8f4355291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] best_iter=80  AUC=0.867039\n",
      "[Fold 2] best_iter=159  AUC=0.817330\n",
      "[Fold 3] best_iter=48  AUC=0.864927\n",
      "[Fold 4] best_iter=227  AUC=0.834122\n",
      "[Fold 5] best_iter=538  AUC=0.837074\n",
      "\n",
      "OOF AUC (XGBoost GPU, rich): 0.837170\n",
      "[Saved Model] XGBmodel_test20250903.joblib\n",
      "[Saved] submission_xgb_gpu_rich.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# XGBoost (GPU) Rich-Features Pipeline\n",
    "#  - Text: Hashing(word 1-2) + Hashing(char_wb 3-5)\n",
    "#  - Example Sims: body ↔ (pos/neg) cosine (해싱 L2 dot)\n",
    "#  - Spam/Pattern: URL/도메인/숫자/대문자비율/키워드\n",
    "#  - Subreddit KFold Target Encoding (누수 방지)\n",
    "#  - (옵션) sentiment probs (train/test_with_sentiment_ctx.csv)\n",
    "#  - Model: XGBoost (binary:logistic), 5-fold OOF AUC + submission\n",
    "# ============================================\n",
    "\n",
    "import os, re, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "import joblib \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "from packaging import version\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Paths\n",
    "# ---------------------------\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "SENT_TRAIN = \"train_with_sentiment_ctx.csv\"   # optional\n",
    "SENT_TEST  = \"test_with_sentiment_ctx.csv\"    # optional\n",
    "OUT_PATH   = \"submission_xgb_gpu_rich.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load & clean\n",
    "# ---------------------------\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "def _clean(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip()\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "for c in [\"body\",\"subreddit\",\"rule\",\n",
    "          \"positive_example_1\",\"positive_example_2\",\n",
    "          \"negative_example_1\",\"negative_example_2\"]:\n",
    "    if c in train.columns: train[c] = train[c].map(_clean)\n",
    "    if c in test.columns:  test[c]  = test[c].map(_clean)\n",
    "\n",
    "assert \"row_id\" in train.columns and \"rule_violation\" in train.columns, \"train.csv에 row_id, rule_violation 필요\"\n",
    "assert \"row_id\" in test.columns, \"test.csv에 row_id 필요\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Combined text → Hashing vectors \n",
    "# ---------------------------\n",
    "def combine_text(df: pd.DataFrame) -> pd.Series:\n",
    "    subr = df.get(\"subreddit\",\"\").astype(str)\n",
    "    rule = df.get(\"rule\",\"\").astype(str)\n",
    "    body = df.get(\"body\",\"\").astype(str)\n",
    "    return (\"subreddit: \" + subr + \" [SEP] rule: \" + rule + \" [SEP] body: \" + body)\n",
    "\n",
    "train_text = combine_text(train)\n",
    "test_text  = combine_text(test)\n",
    "\n",
    "HV_WORD = HashingVectorizer(ngram_range=(1,2), alternate_sign=False, norm=\"l2\", n_features=2**19)\n",
    "HV_CHAR = HashingVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), alternate_sign=False, norm=\"l2\", n_features=2**17)\n",
    "\n",
    "Xw_tr = sp.csr_matrix(HV_WORD.transform(train_text), dtype=np.float32)\n",
    "Xc_tr = sp.csr_matrix(HV_CHAR.transform(train_text), dtype=np.float32)\n",
    "Xw_te = sp.csr_matrix(HV_WORD.transform(test_text),  dtype=np.float32)\n",
    "Xc_te = sp.csr_matrix(HV_CHAR.transform(test_text),  dtype=np.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Example similarity features (cosine via l2-normalized hashing)\n",
    "# ---------------------------\n",
    "def rowwise_cos_by_dot(A, B):\n",
    "    return A.multiply(B).sum(axis=1).A1\n",
    "\n",
    "def example_sims(df: pd.DataFrame):\n",
    "    body = sp.csr_matrix(HV_CHAR.transform(df.get(\"body\",\"\")), dtype=np.float32)\n",
    "    p1   = sp.csr_matrix(HV_CHAR.transform(df.get(\"positive_example_1\",\"\")), dtype=np.float32)\n",
    "    p2   = sp.csr_matrix(HV_CHAR.transform(df.get(\"positive_example_2\",\"\")), dtype=np.float32)\n",
    "    n1   = sp.csr_matrix(HV_CHAR.transform(df.get(\"negative_example_1\",\"\")), dtype=np.float32)\n",
    "    n2   = sp.csr_matrix(HV_CHAR.transform(df.get(\"negative_example_2\",\"\")), dtype=np.float32)\n",
    "    sp1  = rowwise_cos_by_dot(body, p1)\n",
    "    sp2  = rowwise_cos_by_dot(body, p2)\n",
    "    sn1  = rowwise_cos_by_dot(body, n1)\n",
    "    sn2  = rowwise_cos_by_dot(body, n2)\n",
    "    feats = pd.DataFrame({\n",
    "        \"sim_pos1\": sp1, \"sim_pos2\": sp2, \"sim_neg1\": sn1, \"sim_neg2\": sn2,\n",
    "        \"sim_pos_max\": np.maximum(sp1, sp2),\n",
    "        \"sim_neg_max\": np.maximum(sn1, sn2),\n",
    "        \"sim_pos_min\": np.minimum(sp1, sp2),\n",
    "        \"sim_neg_min\": np.minimum(sn1, sn2),\n",
    "        \"sim_pos_avg\": (sp1+sp2)/2.0,\n",
    "        \"sim_neg_avg\": (sn1+sn2)/2.0,\n",
    "        \"sim_pos_minus_neg\": np.maximum(sp1,sp2) - np.maximum(sn1,sn2),\n",
    "    })\n",
    "    return feats\n",
    "\n",
    "sim_tr_df = example_sims(train)\n",
    "sim_te_df = example_sims(test)\n",
    "S_tr = sp.csr_matrix(sim_tr_df.values, dtype=np.float32)\n",
    "S_te = sp.csr_matrix(sim_te_df.values, dtype=np.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Spam/Pattern features \n",
    "# ---------------------------\n",
    "RULE_KEYWORDS = [\n",
    "    r\"spam\", r\"referral\", r\"advertis\", r\"solicit\", r\"promotion\", r\"self[- ]?promo\",\n",
    "    r\"legal\", r\"advice\", r\"nsfw\", r\"porn\", r\"adult\", r\"sexual\",\n",
    "    r\"stream\", r\"watch\", r\"live\", r\"hd\", r\"free\",\n",
    "    r\"torrent\", r\"download\", r\"link\",\n",
    "    r\"scam\", r\"fraud\", r\"giveaway\", r\"bet\", r\"lottery\",\n",
    "    r\"sell\", r\"buy\", r\"trade\",\n",
    "    r\"discord\", r\"telegram\", r\"whatsapp\"\n",
    "]\n",
    "KW_PATTERNS = [re.compile(k, re.I) for k in RULE_KEYWORDS]\n",
    "URL_RE  = re.compile(r\"(https?://|www\\.)\", re.I)\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "DOMAIN_RE = re.compile(r\"\\.[a-z]{2,6}([/ \\n]|$)\", re.I)\n",
    "PHONE_RE  = re.compile(r\"\\+?\\d[\\d\\- ]{7,}\\d\")\n",
    "PRICE_RE  = re.compile(r\"(?:\\$|£|€|₩)\\s?\\d+\")\n",
    "UPPER_RE  = re.compile(r\"[A-Z]\")\n",
    "\n",
    "def body_pattern_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    b = df.get(\"body\",\"\").fillna(\"\")\n",
    "    out = {\n",
    "        \"num_urls\": b.str.count(URL_RE),\n",
    "        \"num_emails\": b.str.count(EMAIL_RE),\n",
    "        \"num_domains\": b.str.count(DOMAIN_RE),\n",
    "        \"num_digits\": b.str.count(r\"\\d\"),\n",
    "        \"num_exclam\": b.str.count(r\"!\"),\n",
    "        \"has_phone\": b.str.contains(PHONE_RE),\n",
    "        \"has_price\": b.str.contains(PRICE_RE),\n",
    "        \"len_chars\": b.str.len(),\n",
    "        \"len_tokens\": b.str.split().map(len),\n",
    "        \"upper_ratio\": b.map(lambda s: (len(UPPER_RE.findall(s)) / max(1, sum(ch.isalpha() for ch in s)))),\n",
    "    }\n",
    "    low = b.str.lower()\n",
    "    for i, pat in enumerate(KW_PATTERNS):\n",
    "        out[f\"kw_{i:02d}\"] = low.str.contains(pat)\n",
    "    return pd.DataFrame(out).fillna(0).astype(float)\n",
    "\n",
    "pat_tr_df = body_pattern_features(train)\n",
    "pat_te_df = body_pattern_features(test)\n",
    "P_tr = sp.csr_matrix(pat_tr_df.values, dtype=np.float32)\n",
    "P_te = sp.csr_matrix(pat_te_df.values, dtype=np.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Subreddit Target Encoding (KFold, leakage-safe)\n",
    "# ---------------------------\n",
    "def kfold_target_encode(train_df, test_df, col, y, n_splits=5, random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    enc = pd.Series(np.zeros(len(train_df), dtype=float), index=train_df.index)\n",
    "    global_mean = float(np.mean(y))\n",
    "    for tr_idx, va_idx in skf.split(train_df, y):\n",
    "        grp = train_df.iloc[tr_idx].groupby(col)[y.name].mean()\n",
    "        enc.iloc[va_idx] = train_df.iloc[va_idx][col].map(grp).fillna(global_mean).values\n",
    "    full_grp = train_df.groupby(col)[y.name].mean()\n",
    "    enc_test = test_df[col].map(full_grp).fillna(global_mean).values\n",
    "    return enc.values.reshape(-1,1), enc_test.reshape(-1,1)\n",
    "\n",
    "y = train[\"rule_violation\"].astype(int)\n",
    "TE_tr_arr, TE_te_arr = kfold_target_encode(train.assign(rule_violation=y),\n",
    "                                           test, col=\"subreddit\", y=y, n_splits=5)\n",
    "TE_tr = sp.csr_matrix(TE_tr_arr.astype(np.float32))\n",
    "TE_te = sp.csr_matrix(TE_te_arr.astype(np.float32))\n",
    "\n",
    "# ---------------------------\n",
    "# 6) (옵션) Sentiment probs 추가\n",
    "# ---------------------------\n",
    "sent_tr_mat = sent_te_mat = None\n",
    "if Path(SENT_TRAIN).exists() and Path(SENT_TEST).exists():\n",
    "    st = pd.read_csv(SENT_TRAIN)[[\"row_id\",\"prob_negative\",\"prob_neutral\",\"prob_positive\"]]\n",
    "    se = pd.read_csv(SENT_TEST)[[\"row_id\",\"prob_negative\",\"prob_neutral\",\"prob_positive\"]]\n",
    "    train = train.merge(st, on=\"row_id\", how=\"left\")\n",
    "    test  = test.merge(se, on=\"row_id\", how=\"left\")\n",
    "    sent_cols = [\"prob_negative\",\"prob_neutral\",\"prob_positive\"]\n",
    "    sent_tr_mat = sp.csr_matrix(train[sent_cols].fillna(0.0).values.astype(np.float32))\n",
    "    sent_te_mat = sp.csr_matrix(test[sent_cols].fillna(0.0).values.astype(np.float32))\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Stack all features\n",
    "# ---------------------------\n",
    "X_tr = sp.hstack([Xw_tr, Xc_tr, S_tr, P_tr, TE_tr] + ([sent_tr_mat] if sent_tr_mat is not None else []),\n",
    "                 format=\"csr\").tocsr()\n",
    "X_te = sp.hstack([Xw_te, Xc_te, S_te, P_te, TE_te] + ([sent_te_mat] if sent_te_mat is not None else []),\n",
    "                 format=\"csr\").tocsr()\n",
    "\n",
    "del Xw_tr, Xc_tr, Xw_te, Xc_te; gc.collect()\n",
    "\n",
    "# ---------------------------\n",
    "# 8) XGBoost (GPU) with 5-fold OOF\n",
    "# ---------------------------\n",
    "pos = y.sum(); neg = len(y) - pos\n",
    "scale_pos_weight = float(neg / max(1, pos))\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    min_child_weight=2.0,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.65,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    n_estimators=4000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "if version.parse(xgb.__version__) >= version.parse(\"2.0.0\"):\n",
    "    xgb_params.update(dict(device=\"cuda\", tree_method=\"hist\"))\n",
    "else:\n",
    "    xgb_params.update(dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\"))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(train), dtype=np.float32)\n",
    "best_rounds = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X_tr, y), 1):\n",
    "    X_tr_f, X_va_f = X_tr[tr_idx], X_tr[va_idx]\n",
    "    y_tr_f, y_va_f = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "    # Create the EarlyStopping object\n",
    "    early_stop = EarlyStopping(rounds=200, save_best=True)\n",
    "    \n",
    "    # Pass callbacks to the classifier's CONSTRUCTOR\n",
    "    clf = XGBClassifier(**xgb_params, callbacks=[early_stop])\n",
    "    \n",
    "    # Fit the model\n",
    "    clf.fit(\n",
    "        X_tr_f, y_tr_f,\n",
    "        eval_set=[(X_va_f, y_va_f)],\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    oof[va_idx] = clf.predict_proba(X_va_f)[:,1]\n",
    "    br = clf.best_iteration\n",
    "    best_rounds.append(br)\n",
    "    print(f\"[Fold {fold}] best_iter={best_rounds[-1]}  AUC={roc_auc_score(y_va_f, oof[va_idx]):.6f}\")\n",
    "\n",
    "oof_auc = roc_auc_score(y, oof)\n",
    "print(f\"\\nOOF AUC (XGBoost GPU, rich): {oof_auc:.6f}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Full fit & predict test\n",
    "# ---------------------------\n",
    "best_n = int(np.median([r for r in best_rounds if r is not None])) if best_rounds else 1200\n",
    "clf_full = XGBClassifier(**{**xgb_params, \"n_estimators\": best_n})\n",
    "clf_full.fit(X_tr, y, verbose=False)\n",
    "\n",
    "# 모델 저장\n",
    "import joblib \n",
    "MODEL_FILE = \"XGBmodel_test20250903.joblib\"\n",
    "\n",
    "# 마지막 fold 모델(clf) 대신 전체 데이터로 학습한 모델(clf_full)을 저장합니다.\n",
    "joblib.dump(clf_full, MODEL_FILE)\n",
    "print(f\"[Saved Model] {MODEL_FILE}\")\n",
    "\n",
    "\n",
    "test_proba = clf_full.predict_proba(X_te)[:,1]\n",
    "pd.DataFrame({\"row_id\": test[\"row_id\"], \"rule_violation\": test_proba}).to_csv(OUT_PATH, index=False)\n",
    "print(f\"[Saved] {OUT_PATH}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
